{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "from glob import glob\n",
    "import tqdm\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "import torchtext.datasets as datasets\n",
    "from torchtext.legacy import data, vocab\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "LOGGER = logging.getLogger(\"toxic_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_csv(train_csv, test_csv, split=0.2, seed=999):\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "        \n",
    "    # read train csv file\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    df_train[\"comment_text\"] = df_train.comment_text.str.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # create validation data\n",
    "    idx = np.arange(df_train.shape[0])\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(idx)\n",
    "    val_size = int(len(idx) * split)\n",
    "    df_train.iloc[idx[val_size:], :].to_csv(\"data/dataset_train.csv\", index=False)\n",
    "    df_train.iloc[idx[:val_size], :].to_csv(\"data/dataset_val.csv\", index=False)\n",
    "    \n",
    "    # read test csv file\n",
    "    df_test = pd.read_csv(test_csv)\n",
    "    df_test[\"comment_text\"] = df_test.comment_text.str.replace(\"\\n\", \" \")\n",
    "    df_test.to_csv(\"data/dataset_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Comments Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/razavi/DATA//datasets/kaggle/toxic_comments'\n",
    "\n",
    "train_csv = f'{data_dir}/train.csv'\n",
    "test_csv = f'{data_dir}/test.csv'\n",
    "\n",
    "# batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove', 'jigsaw', 'sample_submission.csv', 'sample_submission.csv.zip', 'test.csv', 'test.csv.zip', 'train.csv', 'train.csv.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76118</th>\n",
       "      <td>792929352212</td>\n",
       "      <td>\", 23 May 2010 (UTC)\\n\\nMy best advice to you ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46522</th>\n",
       "      <td>485624205310</td>\n",
       "      <td>\"\\n OK, here's the deal: First, the original c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46730</th>\n",
       "      <td>487777142271</td>\n",
       "      <td>Then this will never be a neutral article, bec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45640</th>\n",
       "      <td>476531125854</td>\n",
       "      <td>So you can print it out after?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91445</th>\n",
       "      <td>955122798306</td>\n",
       "      <td>Welcome to Wikipedia! We welcome your help to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "76118  792929352212  \", 23 May 2010 (UTC)\\n\\nMy best advice to you ...      0   \n",
       "46522  485624205310  \"\\n OK, here's the deal: First, the original c...      0   \n",
       "46730  487777142271  Then this will never be a neutral article, bec...      1   \n",
       "45640  476531125854                     So you can print it out after?      0   \n",
       "91445  955122798306  Welcome to Wikipedia! We welcome your help to ...      0   \n",
       "\n",
       "       severe_toxic  obscene  threat  insult  identity_hate  \n",
       "76118             0        0       0       0              0  \n",
       "46522             0        0       0       0              0  \n",
       "46730             0        0       0       0              0  \n",
       "45640             0        0       0       0              0  \n",
       "91445             0        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.sample(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = 0\n",
    "eos_token = 1\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"<sos>\": 0, \"<eos>\": 1}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.count = 2\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2index:\n",
    "            self.word2index[word] = self.count\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.count] = word\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2498644d34df4d2cb62244812c72d9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building vocabulary:   0%|          | 0/95851 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "all_comments_text = train_df[\"comment_text\"]\n",
    "for text in tqdm.notebook.tqdm(all_comments_text, desc='Building vocabulary'):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        vocab.add_sentence(sent)\n",
    "        \n",
    "with open('vocab.pkl', 'bw') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open('vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400126\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 95851\n",
      "toxic: 9237\n",
      "severe_toxic: 965\n",
      "obscene: 5109\n",
      "threat: 305\n",
      "insult: 4765\n",
      "identity_hate: 814\n"
     ]
    }
   ],
   "source": [
    "print(\"All:\", len(train_df))\n",
    "print(\"toxic:\", len(train_df[train_df['toxic'] == 1]))\n",
    "print(\"severe_toxic:\", len(train_df[train_df['severe_toxic'] == 1]))\n",
    "print(\"obscene:\", len(train_df[train_df['obscene'] == 1]))\n",
    "print(\"threat:\", len(train_df[train_df['threat'] == 1]))\n",
    "print(\"insult:\", len(train_df[train_df['insult'] == 1]))\n",
    "print(\"identity_hate:\", len(train_df[train_df['identity_hate'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>conda install -c conda-forge spacy</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = spacy.load('en_core_web_sm')\n",
    "MAX_CHARS = 20000\n",
    "\n",
    "def tokenizer(comment):\n",
    "    comment = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "        str(comment))\n",
    "    comment = re.sub(r\"[ ]+\", \" \", comment)\n",
    "    comment = re.sub(r\"\\!+\", \"!\", comment)\n",
    "    comment = re.sub(r\"\\,+\", \",\", comment)\n",
    "    comment = re.sub(r\"\\?+\", \"?\", comment)\n",
    "    if (len(comment) > MAX_CHARS):\n",
    "        comment = comment[:MAX_CHARS]\n",
    "    return [x.text for x in NLP.tokenizer(comment) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(train_scv, test_csv, split=0.2, fix_length=100, lower=False, vectors=None):\n",
    "    if vectors is not None:\n",
    "        # pretrain vectors only supports all lower cases\n",
    "        lower = True\n",
    "    \n",
    "    LOGGER.debug(\"Preparing CSV files...\")\n",
    "#     prepare_csv(train_csv, test_csv, split)\n",
    "    \n",
    "    comment = data.Field(\n",
    "        sequential=True,\n",
    "        fix_length=fix_length,\n",
    "        tokenize=tokenizer,\n",
    "        pad_first=True,\n",
    "#         tensor_type=torch.cuda.LongTensor,\n",
    "        lower=lower\n",
    "    )\n",
    "    \n",
    "    print(\"Reading train csv file...\")\n",
    "    train, val = data.TabularDataset.splits(\n",
    "        path=data_dir, format='csv', skip_header=True,\n",
    "        train='train.csv', validation='test.csv',\n",
    "        fields=[\n",
    "            ('id', None),\n",
    "            ('comment_text', comment),\n",
    "            ('toxic', data.Field(\n",
    "                use_vocab=False, sequential=False)),\n",
    "            ('severe_toxic', data.Field(\n",
    "                use_vocab=False, sequential=False, \n",
    "                )),\n",
    "            ('obscene', data.Field(\n",
    "                use_vocab=False, sequential=False, \n",
    "                )),\n",
    "            ('threat', data.Field(\n",
    "                use_vocab=False, sequential=False, \n",
    "                )),\n",
    "            ('insult', data.Field(\n",
    "                use_vocab=False, sequential=False, \n",
    "                )),\n",
    "            ('identity_hate', data.Field(\n",
    "                use_vocab=False, sequential=False, \n",
    "                )),\n",
    "        ])\n",
    "    \n",
    "    print(\"Reading test csv file...\")\n",
    "    test = data.TabularDataset(\n",
    "        path=f'{data_dir}/test.csv', format='csv', \n",
    "        skip_header=True,\n",
    "        fields=[\n",
    "            ('id', None),\n",
    "            ('comment_text', comment)\n",
    "        ])\n",
    "    \n",
    "    print(\"Building vocabulary...\")\n",
    "    comment.build_vocab(\n",
    "        train, val, test,\n",
    "        max_size=20000,\n",
    "        min_freq=50,\n",
    "        vectors=vectors\n",
    "    )\n",
    "    \n",
    "    print(\"Done preparing the datasets\")\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train csv file...\n",
      "Reading test csv file...\n",
      "Building vocabulary...\n",
      "Done preparing the datasets\n",
      "CPU times: user 1min 37s, sys: 1.13 s, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds, valid_ds, test_ds = get_dataset(train_csv, test_csv, split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95851\n",
      "153164\n",
      "153164\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds.examples))\n",
    "print(len(valid_ds.examples))\n",
    "print(len(test_ds.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'comment_text': <torchtext.legacy.data.field.Field at 0x7f0f0d7c6f28>,\n",
       " 'toxic': <torchtext.legacy.data.field.Field at 0x7f0f0d7c60b8>,\n",
       " 'severe_toxic': <torchtext.legacy.data.field.Field at 0x7f0f0d7c6da0>,\n",
       " 'obscene': <torchtext.legacy.data.field.Field at 0x7f0f0d7c6e48>,\n",
       " 'threat': <torchtext.legacy.data.field.Field at 0x7f0f0d7c6550>,\n",
       " 'insult': <torchtext.legacy.data.field.Field at 0x7f0f0d7c6f98>,\n",
       " 'identity_hate': <torchtext.legacy.data.field.Field at 0x7f0f0d7c6b38>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95851\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iterator(dataset, batch_size, train=True, \n",
    "    shuffle=True, repeat=False):\n",
    "    dataset_iter = data.Iterator(\n",
    "        dataset, batch_size=batch_size, device=torch.device('cuda'),\n",
    "        train=train, shuffle=shuffle, repeat=repeat,\n",
    "        sort=False\n",
    "    )\n",
    "    return dataset_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iter = get_iterator(train_ds, batch_size, train=True, shuffle=True, repeat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,     1,     0,  ...,     0,     1,     0],\n",
      "        [    1,     1,    10,  ...,     0,     1,     6],\n",
      "        [    1,     1,     8,  ...,     7,     1,   116],\n",
      "        ...,\n",
      "        [ 4532, 18139,  2026,  ...,     0,     6,   818],\n",
      "        [ 1339,   576,   641,  ...,     3,  1946,  4116],\n",
      "        [  256,     2,  1227,  ...,    47,     2,  2135]], device='cuda:0')\n",
      "tensor([[0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [ 117,  277, 1306,  ...,    7,   30,  515],\n",
      "        [ 135,  235,    2,  ...,  504,  400,  498],\n",
      "        [   2,    2,    0,  ...,   11,    2,    2]], device='cuda:0')\n",
      "tensor([[0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i, examples in enumerate(train_iter):\n",
    "    x = examples.comment_text # (fix_length, batch_size) Tensor\n",
    "    y = torch.stack([\n",
    "        examples.toxic, \n",
    "        examples.severe_toxic, \n",
    "        examples.obscene,\n",
    "        examples.threat, \n",
    "        examples.insult, \n",
    "        examples.identity_hate\n",
    "    ], dim=1)\n",
    "    \n",
    "    print(x)\n",
    "    print(y)\n",
    "    if i >= 1: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.word_em.weight.data = train_dataset.fields[\"comment_text\"].vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes=6, num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size, )\n",
    "        self.gru = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, bidirectional=True, dropout=0.5)\n",
    "        self.out = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.embedding(x)\n",
    "        output,(h, c) = self.gru(output)\n",
    "        output = self.out(h[-1].unsqueeze(1))\n",
    "        output = F.relu(output)\n",
    "        output = F.dropout(output, p=0.1)\n",
    "        output = F.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20002\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "vocab = train_ds.fields['comment_text'].vocab\n",
    "print(len(vocab))\n",
    "model = EncoderRNN(len(vocab), hidden_size, num_classes=6, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embedding): Embedding(20002, 128)\n",
       "  (gru): LSTM(128, 128, dropout=0.5, bidirectional=True)\n",
       "  (out): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5b0bbd148448edb897d780b4a5f5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1, 1, 384])) that is different to the input size (torch.Size([64, 1, 6])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-36abcc8fdce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# backward step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/mygithub/DeepLearning/Deep_Learning_in_Python_2018/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/mygithub/DeepLearning/Deep_Learning_in_Python_2018/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/mygithub/DeepLearning/Deep_Learning_in_Python_2018/venv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2884\u001b[0m         raise ValueError(\n\u001b[1;32m   2885\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2886\u001b[0;31m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2887\u001b[0m         )\n\u001b[1;32m   2888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([1, 1, 384])) that is different to the input size (torch.Size([64, 1, 6])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "#     h = to_var(torch.zeros((num_layers, batch_size, hidden_size)))\n",
    "    \n",
    "    for i, examples in tqdm.notebook.tqdm(enumerate(train_iter)):\n",
    "        x = examples.comment_text\n",
    "        y = torch.stack([examples.toxic, examples.severe_toxic, examples.obscene,\n",
    "                         examples.threat, examples.insult, examples.identity_hate], dim=1)\n",
    "        \n",
    "        # forward step\n",
    "        output = model(x)\n",
    "        \n",
    "        # loss\n",
    "        loss = criterion(output, y.float().view(1, 1, -1))\n",
    "        \n",
    "        # backward step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # stats\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write('\\r loss = {:.5f}'.format(loss.item()))\n",
    "        \n",
    "        if i > len(train_ds.examples): break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, DATA_DIR, filenames):\n",
    "        self.vocab = Vocabulary()\n",
    "        self.data = self.tokenize(DATA_DIR, filenames)\n",
    "\n",
    "    def tokenize(self, DATA_DIR, filenames):\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(DATA_DIR, filename)\n",
    "            with open(path, 'r') as f:\n",
    "                tokens = 0\n",
    "                for line in f:\n",
    "                    words = line.split() + ['<eos>']\n",
    "                    tokens += len(words)\n",
    "                    for word in words:\n",
    "                        self.vocab.add_word(word)\n",
    "\n",
    "            # Tokenize file content\n",
    "            with open(path, 'r') as f:\n",
    "                ids = torch.LongTensor(tokens)\n",
    "                token = 0\n",
    "                for line in f:\n",
    "                    words = line.split() + ['<eos>']\n",
    "                    for word in words:\n",
    "                        ids[token] = self.dictionary.word2idx[word]\n",
    "                        token += 1\n",
    "\n",
    "        return ids\n",
    "\n",
    "class TxtDatasetProcessing(Dataset):\n",
    "    def __init__(self, data_path, txt_path, txt_filename, label_filename, sen_len, corpus):\n",
    "        self.txt_path = os.path.join(data_path, txt_path)\n",
    "        # reading txt file from file\n",
    "        txt_filepath = os.path.join(data_path, txt_filename)\n",
    "        fp = open(txt_filepath, 'r')\n",
    "        self.txt_filename = [x.strip() for x in fp]\n",
    "        fp.close()\n",
    "        # reading labels from file\n",
    "        label_filepath = os.path.join(data_path, label_filename)\n",
    "        fp_label = open(label_filepath, 'r')\n",
    "        labels = [int(x.strip()) for x in fp_label]\n",
    "        fp_label.close()\n",
    "        self.label = labels\n",
    "        self.corpus = corpus\n",
    "        self.sen_len = sen_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = os.path.join(self.txt_path, self.txt_filename[index])\n",
    "        fp = open(filename, 'r')\n",
    "        txt = torch.LongTensor(np.zeros(self.sen_len, dtype=np.int64))\n",
    "        count = 0\n",
    "        clip = False\n",
    "        for words in fp:\n",
    "            for word in words.split():\n",
    "                if word.strip() in self.corpus.dictionary.word2idx:\n",
    "                    if count > self.sen_len - 1:\n",
    "                        clip = True\n",
    "                        break\n",
    "                    txt[count] = self.corpus.dictionary.word2idx[word.strip()]\n",
    "                    count += 1\n",
    "            if clip: break\n",
    "        label = torch.LongTensor([self.label[index]])\n",
    "        return txt, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.txt_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h0 = to_var(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = to_var(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return h0, c0\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), self.batch_size, -1)\n",
    "        output, self.hidden = self.lstm(x, self.hidden)\n",
    "        y = self.hidden2label(output[-1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter setting\n",
    "epochs = 50\n",
    "batch_size = 5\n",
    "learning_rate = 0.002\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 50\n",
    "seq_len = 100\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "\n",
    "# test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(embedding_dim=embedding_dim, hidden_dim=hidden_dim, \n",
    "                       vocab_size=len(vocab), label_size=num_classes, \n",
    "                       batch_size=batch_size)\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "if use_gpu:\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "train_acc_ = []\n",
    "test_acc_ = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    ## training epoch\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    for i, traindata in enumerate(train_loader):\n",
    "        train_inputs, train_labels = traindata\n",
    "        train_labels = torch.squeeze(train_labels)\n",
    "\n",
    "        if use_gpu:\n",
    "            train_inputs, train_labels = Variable(train_inputs.cuda()), train_labels.cuda()\n",
    "        else: train_inputs = Variable(train_inputs)\n",
    "\n",
    "        model.zero_grad()\n",
    "        model.batch_size = len(train_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(train_inputs.t())\n",
    "\n",
    "        loss = loss_function(output, Variable(train_labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calc training acc\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == train_labels).sum()\n",
    "        total += len(train_labels)\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "    train_loss_.append(total_loss / total)\n",
    "    train_acc_.append(total_acc / total)\n",
    "    ## testing epoch\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    for iter, testdata in enumerate(test_loader):\n",
    "        test_inputs, test_labels = testdata\n",
    "        test_labels = torch.squeeze(test_labels)\n",
    "\n",
    "        if use_gpu:\n",
    "            test_inputs, test_labels = Variable(test_inputs.cuda()), test_labels.cuda()\n",
    "        else: test_inputs = Variable(test_inputs)\n",
    "\n",
    "        model.batch_size = len(test_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(test_inputs.t())\n",
    "\n",
    "        loss = loss_function(output, Variable(test_labels))\n",
    "\n",
    "        # calc testing acc\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == test_labels).sum()\n",
    "        total += len(test_labels)\n",
    "        total_loss += loss.data[0]\n",
    "    test_loss_.append(total_loss / total)\n",
    "    test_acc_.append(total_acc / total)\n",
    "\n",
    "    print('[Epoch: %3d/%3d] Training Loss: %.3f, Testing Loss: %.3f, Training Acc: %.3f, Testing Acc: %.3f'\n",
    "          % (epoch, epochs, train_loss_[epoch], test_loss_[epoch], train_acc_[epoch], test_acc_[epoch]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
